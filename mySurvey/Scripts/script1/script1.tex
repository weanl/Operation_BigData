\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}

\usepackage{url}
\usepackage{enumerate}

\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}
\title{综述. 特征选择方法}
\author{MF1733062 万晨 \url{weanl_jc@163.com}}
\maketitle

\section*{1. 介绍}
  FS在ML中能解决那些问题？已经解决得怎么样了？还有那些问题？feature construction = FS + FE
  特征选择(Feature, Variable and Attribution Selection)，是机器学习中feature construction的重要组成部分。
  在筛选原始数据，构造有用的特征集合方面，特征选择不同于特征提取(Feature Extraction)：后者会通过线性或非线性的方式从原始数据中
  构造出全新的特征，具有特征学习和表示学习的能力["Representation Learning: A Review and New Perspectives"]；
  特征选择通过设计一些简单高效或者精致巧妙的方法，实现从原始特征集合中选出最优的特征子集，能够保持特征对应的原始物理意义。
  所谓最优特征子集，理论上定义为没有信息丢失的最小特征子集，以Markov blanket的形式给出[D. Koller, Toward optimal feature selection]
  [C.Aliferis, Local causal and markov blanket induction for causal discovery and feature selection]；
  实际中理论的ground-truth很难找，所以经验上一般我们用预测器性能（如分类器的精度）来评估特征子集的选择结果。
  特征选择一直以来是一个重要的课题：为了分析特征间相关性，早期[Blum and langley,1997,Kohavi and John,1997]等
  在1997年提出了特征选择方面课题研究，当时大多数应用领域下特征维数还不超过40。后续基因序列分析和web文本分类等典型应用不断推动特征选择课题研究：
  [2001, Feature selection for high-dimensional genomic microarray data]针对高维的染色体序列数据提出了给予特征选择的基因分析方法，
  [2015, Deep Feature Selection: Theory and Application to Identify Enhancer and Promoters]将深层神经网络应用到特征选择中，实现了
  基因Enhancer和Promoter的有效分析。目前特征选择研究有两大趋势：第一，应对各种结构化和非结构化的数据设计出一套较为通用的方法，
  决策树类和深层神经网络类在特征选择方面的改进是不错的解决方法，[Feature Selection via Regularized Trees]就是通过修改单棵树的构造算法
  实现基于随机森林的Ensemble类的特征选择方法；第二，应对curse of dimensionality，设计复杂度较低的算法，有效地处理高维数据，改进现有的算法
  以及组合使用一些简单的算法都是很好的思路。

  在数据处理中，应用特征选择方法概括起来有如下优势：
  \begin{itemize}
    \item 可以过滤无关特征：
    采集过程可能引入数据噪声，从而影响后续的数据处理；同样与任务显著的无关特征(irrelevant)也可以认为是噪声，特征选择方法一定程度上可以过滤这一部分噪声；

    \item 可以剔除冗余特征：
    相当部分特征虽然与任务相关，但互相之间存在显著的冗余关系(redundant)，特征选择方法可以依据实际需求选出代表性的特征，降低冗余；

    \item 可以实现特征重要性的评估：
    一些带有指标（如相关系数、权值）或其他“得分”的特征选择方法，可以在选出的特征子集中按指标或“得分”对特征进行重要性排序。

  \end{itemize}

  接下来的部分，我们会分别介绍特征选择典型的三种分类：过滤式(filter)、包裹式(wrapper)和嵌入式(embbedding)。

  过滤式：（线性相关系数、互信息系数、relief and relief-F、combination）

  包裹式：（一般子集搜索策略）





\section*{2. 过滤式与包裹式}
  特征选择的形式化定义：
  首先将样本空间定义为一个$ N $维的随机变量$ X = (X_{0}, X_{1}, ... , X_{N-1})$，其服从一定的概率分布，通常认为样本实例（数据）都是该概率分布采样所得；
  特征全集$ S = \{X_{0}, X_{1}, ... , X_{N-1}\} $，特征选择考虑的是如何有效地除去无关和冗余的特征，筛选得到”最优”特征子集$ S^{*} \subset S $[?]。
  显然，暴力地遍历$ S $所有子集会遇到组合爆炸，是不可行的。实际设计中，常采用如下思路进行搜索：

  \begin{itemize}

    \item 产生初始候选特征子集，进行下一步；
    \item 评价候选特征子集，如果达到终止条件终止搜索，否则进行下一步；
    \item 基于上一步评价结果，生成新候选特征子集，返回上一步。

  \end{itemize}
  由上，特征选择方法可以依据新候选特征子集的方法(Search Strategies)进行分类，通常有Complete、Sequential和Random等策略，但这种分类方法不是主流的
  划分方法。主流的分类方法是依据候选子集的评价方法(Evaluation Criteria)对特征选择方法进行分类，简单地说，用独立于学习器的方法进行评价的称为过滤式方法，
  用学习器的性能进行评价的称为包裹式方法，本章节主要介绍这两种很早被提出的方法。下一章节我们还会介绍后续巧妙独特的嵌入式方法，不同于前两种前两类方法，
  嵌入式方法将特征选择嵌入到学习器的训练过程中。


\subsection*{2.1 过滤式方法}
  理论定义上：

  \begin{algorithm}
    \caption{Filter Algorithm}
    \begin{algorithmic}
      \REQUIRE $ D(F_{0}, F_{1},..., F_{N-1})$

               $ S_{0}$
      \ENSURE $ S_{best} $

    \end{algorithmic}




  \end{algorithm}

  算法中$ eval(S_{0}, D, M)$说明过滤式采用独立的评价方法，下面介绍几种常见的方法。

\subsubsection*{2.1.1 ********}
  线性相关系数、信息熵方法及MIC、relief-F

  在统计学中，常用线性相关系数(Pearson correlation coefficient)衡量两个随机变量间线性相关性。若定义两个随机变量$ A, B $,则可有$M$个采样
  $ A: a_{0},...,a_{M-1}; B: b_{0},...,b_{M-1}$计算出两随机变量间的线性相关系数（的估计值）：
  $$  r_{AB} = \frac{\sum_{i=0}^{M-1}(a_{i}-\overline{a})(b_{i}-\overline{b})}{\sqrt{\sum_{i=0}^{M-1}(a_{i}-\overline{a})^2}\sqrt{\sum_{i=0}^{M-1}(b_{i}-\overline{b})^2}}$$

  $ r_{AB} $在$ [-1,1]$上取值，一般做如下判断：如果$ \mid{r_{AB}}\mid<0.2$,认为A和B显著地没有线性相关性；如果$ \mid{r_{AB}}\mid>0.8$,认为A和B显著地有线性相关性。
  在特征选择中一般做如下应用：令$ A=X_{i},i=0,1,...,N-1, B=Y$，其中$Y$为监督学习下给出的数据标签对应的随机变量称为标签变量，满足现实的映射关系$Y=f(X)$；然后计算$ r_{AB} $，
  即计算$N$个特征和标签变量的线性相关系数，从而剔除一些$\mid{r_{AB}}\mid$过小的特征。该方法在线性假设下能过滤一些无关特征，计算简单应用广泛。
  这里需要注意的是该方法由如下应用前提：$A,B$为连续的随机变量；$A,B$应通过正太分布检验，即大致呈正太分布。



\subsubsection*{2.1.2 组合方法}

[2003, Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution] FCBC Algorithm



\subsection*{2.2 包裹式方法}

\begin{algorithm}
  \caption{Wrapper Algorithm}
  \begin{algorithmic}
    \REQUIRE $ D(F_{0}, F_{1},..., F_{N-1})$

             $ S_{0}$
    \ENSURE $ S_{best} $

  \end{algorithmic}




\end{algorithm}

包裹式方法用学习器的性能对候选特征子集进行评价，好处在于



\section*{3. 嵌入式}

















\end{document}
